# -*- coding: utf-8 -*-
"""DublinHousePricePrediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J46GErBgWDYZNFbfF793G8gDyeDXzHAS
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
from scipy.stats import norm
from scipy.special import boxcox1p
import missingno as msno
from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split
from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.pipeline import make_pipeline
from sklearn.svm import SVR
from xgboost import XGBRegressor
import re

import warnings
warnings.filterwarnings('ignore')

# def ignore_warn(*args, **kwargs):
#     pass
# warnings.warn = ignore_warn

pd.set_option("display.float_format", lambda x: "{:.3f}".format(x)) #Limiting floats output to 3 decimal points
pd.set_option("display.max_columns", None)

# Importing dataset and examining it
dataset = pd.read_csv("/content/drive/MyDrive/DublinHousePrice.csv")
pd.set_option('display.max_columns', None) # to make sure you can see all the columns in output window
print(dataset.head())
print(dataset.shape)
print(dataset.info())
print(dataset.describe())

dataset.isnull().sum()

# drop the last column
dataset = dataset.drop([ "title"], axis=1)

dataset.info()
dataset.price.describe()

sns.swarmplot(dataset.price)

sns.distplot(dataset.price, bins=50)

print(dataset.price.skew())
print(dataset.price.kurt())

fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(8,4))
sns.distplot(ax=axs[0], x=dataset['price'], fit=norm);
stats.probplot(x=dataset['price'], plot=plt)
plt.show()

# check correlation between independent variables with price
matrix = dataset.corr()
mask = np.triu(np.ones_like(matrix, dtype=bool))
cmap = sns.diverging_palette(220, 25, s=80, n=9, as_cmap=True, center="light")
plt.figure(figsize=(8,8))
sns.heatmap(matrix, mask = mask, annot=True, cmap=cmap, square=True, fmt='.2f',linewidth=.2, center=0, vmin=-0.15, vmax=0.55)
plt.show()

sns.scatterplot([dataset.propertySize, dataset.price]) # one outlier on the right bottom corner

sns.boxplot([dataset.numBedrooms, dataset.price])

fig, axes = plt.subplots(1, 3, figsize=(16, 6))
fig.suptitle('Categorical columns vs Price')

sns.swarmplot(ax=axes[0],data = dataset, x = "featuredLevel", y = "price", order=["basic", "standard", "premium"])
sns.swarmplot(ax=axes[1],data = dataset, x = "category", y = "price")  # houses under NewHomes menu has higher price than under Buy menu in the daft.ie website

plt.xticks(rotation=45)
plt.show()

# Impute missing values for numerical features with mean
numerical_features = dataset.select_dtypes(include=[np.number])
numerical_features = numerical_features.fillna(numerical_features.mean())

# Impute missing values for categorical features with the most frequent category
categorical_features = dataset.select_dtypes(include=[np.object])
categorical_features = categorical_features.fillna(categorical_features.mode().iloc[0])

# Check for outliers in numerical features and remove if necessary
# (you may need to set appropriate threshold values based on your data)
for column in numerical_features.columns:
    q1 = dataset[column].quantile(0.25)
    q3 = dataset[column].quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    dataset = dataset[(dataset[column] >= lower_bound) & (dataset[column] <= upper_bound)]

# Additional data cleaning steps (if required)
# For example, removing rows with specific conditions
# dataset = dataset[dataset['column_name'] != 'unwanted_value']

# Example: Creating a new feature by combining existing features
dataset['total_rooms'] = dataset['numBedrooms'] + dataset['numBathrooms']

# Example: Applying a logarithmic transformation to a feature
dataset['log_price'] = np.log1p(dataset['price'])

# Example: Creating dummy variables for categorical features
categorical_features = dataset.select_dtypes(include=[np.object])
categorical_features = pd.get_dummies(categorical_features, drop_first=True)
dataset = pd.concat([dataset, categorical_features], axis=1)

# 1. Identify Numerical Features
numerical_features = ['numBedrooms', 'numBathrooms', 'propertySize', 'm_totalImages', 'longitude', 'latitude']

# 2. Identify Categorical Features
categorical_features = ['featuredLevel', 'propertyType', 'category', 'ber_rating']

# 3. Check Correlations
# Calculate correlations with the target variable 'price'
correlations = dataset.corr()['price'].abs().sort_values(ascending=False)

# Select numerical features with a correlation above a certain threshold (e.g., 0.3)
correlation_threshold = 0.3
selected_numerical_features = correlations[correlations > correlation_threshold].index.tolist()

# Combine selected numerical and categorical features
selected_feature_names = selected_numerical_features + categorical_features

# The 'selected_feature_names' variable now contains the names of the features that you have selected based on correlations and domain knowledge.

X = dataset[selected_feature_names]
y = dataset['price']

# Encode categorical features using one-hot encoding
categorical_features = ['featuredLevel', 'propertyType', 'category', 'ber_rating',]
dataset_encoded = pd.get_dummies(dataset, columns=categorical_features, drop_first=True)

# Data Splitting
# Assume 'price' is the target variable
X = dataset_encoded.drop(['price', 'publishDate'], axis = 1)  # Features
y = dataset_encoded['price']  # Target variable

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR



# Hyperparameters for each model
linear_params = {}
lasso_params = {'alpha': [0.01, 0.1, 1.0, 10.0]}
ridge_params = {'alpha': [0.01, 0.1, 1.0, 10.0]}
elastic_net_params = {'alpha': [0.01, 0.1, 1.0, 10.0], 'l1_ratio': [0.1, 0.5, 0.7, 0.9]}
rf_params = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20], 'min_samples_split': [2, 5, 10]}
svr_params = {'kernel': ['linear', 'poly', 'rbf'], 'C': [0.1, 1.0, 10.0], 'gamma': ['scale', 'auto']}

# Create instances of each model
linear_model = LinearRegression()
lasso_model = Lasso()
ridge_model = Ridge()
elastic_net_model = ElasticNet()
random_forest_model = RandomForestRegressor()
svr_model = SVR()

# Perform Grid Search with Cross-Validation for each model
linear_grid_search = GridSearchCV(linear_model, param_grid=linear_params, cv=5)
lasso_grid_search = GridSearchCV(lasso_model, param_grid=lasso_params, cv=5)
ridge_grid_search = GridSearchCV(ridge_model, param_grid=ridge_params, cv=5)
elastic_net_grid_search = GridSearchCV(elastic_net_model, param_grid=elastic_net_params, cv=5)
rf_grid_search = GridSearchCV(random_forest_model, param_grid=rf_params, cv=5)
svr_grid_search = GridSearchCV(svr_model, param_grid=svr_params, cv=5)

# Fit each model to the data with Grid Search
linear_grid_search.fit(X_train, y_train)
lasso_grid_search.fit(X_train, y_train)
ridge_grid_search.fit(X_train, y_train)
elastic_net_grid_search.fit(X_train, y_train)
rf_grid_search.fit(X_train, y_train)
svr_grid_search.fit(X_train, y_train)

# Print the best hyperparameters for each model
print("Best hyperparameters for Linear Regression:", linear_grid_search.best_params_)
print("Best hyperparameters for Lasso:", lasso_grid_search.best_params_)
print("Best hyperparameters for Ridge:", ridge_grid_search.best_params_)
print("Best hyperparameters for ElasticNet:", elastic_net_grid_search.best_params_)
print("Best hyperparameters for Random Forest:", rf_grid_search.best_params_)
print("Best hyperparameters for SVR:", svr_grid_search.best_params_)

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error



# Get the best models from GridSearchCV
best_linear_model = linear_grid_search.best_estimator_
best_lasso_model = lasso_grid_search.best_estimator_
best_ridge_model = ridge_grid_search.best_estimator_
best_elastic_net_model = elastic_net_grid_search.best_estimator_
best_rf_model = rf_grid_search.best_estimator_
best_svr_model = svr_grid_search.best_estimator_

# Make predictions on the test set using the best models
y_pred_linear = best_linear_model.predict(X_test)
y_pred_lasso = best_lasso_model.predict(X_test)
y_pred_ridge = best_ridge_model.predict(X_test)
y_pred_elastic_net = best_elastic_net_model.predict(X_test)
y_pred_rf = best_rf_model.predict(X_test)
y_pred_svr = best_svr_model.predict(X_test)

# Calculate performance scores for each model
mse_linear = mean_squared_error(y_test, y_pred_linear)
r2_linear = r2_score(y_test, y_pred_linear)
mae_linear = mean_absolute_error(y_test, y_pred_linear)
adj_r2_linear = 1 - (1 - r2_linear) * ((len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1))

mse_lasso = mean_squared_error(y_test, y_pred_lasso)
r2_lasso = r2_score(y_test, y_pred_lasso)
mae_lasso = mean_absolute_error(y_test, y_pred_lasso)
adj_r2_lasso = 1 - (1 - r2_lasso) * ((len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1))

mse_ridge = mean_squared_error(y_test, y_pred_ridge)
r2_ridge = r2_score(y_test, y_pred_ridge)
mae_ridge = mean_absolute_error(y_test, y_pred_ridge)
adj_r2_ridge = 1 - (1 - r2_ridge) * ((len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1))

mse_elastic_net = mean_squared_error(y_test, y_pred_elastic_net)
r2_elastic_net = r2_score(y_test, y_pred_elastic_net)
mae_elastic_net = mean_absolute_error(y_test, y_pred_elastic_net)
adj_r2_elastic_net = 1 - (1 - r2_elastic_net) * ((len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1))

mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)
mae_rf = mean_absolute_error(y_test, y_pred_rf)
adj_r2_rf = 1 - (1 - r2_rf) * ((len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1))


mse_svr = mean_squared_error(y_test, y_pred_svr)
r2_svr = r2_score(y_test, y_pred_svr)
mae_svr = mean_absolute_error(y_test, y_pred_svr)
adj_r2_svr = 1 - (1 - r2_svr) * ((len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1))

# Print the performance scores
print("Linear Regression - MSE:", mse_linear, "MAE:", mae_linear, "R-squared:", r2_linear, "Adj R-squared:", adj_r2_linear)
print("Lasso Regression - MSE:", mse_lasso, "MAE:", mae_lasso, "R-squared:", r2_lasso, "Adj R-squared:", adj_r2_lasso)
print("Ridge Regression - MSE:", mse_ridge, "MAE:", mae_ridge, "R-squared:", r2_ridge, "Adj R-squared:", adj_r2_ridge)
print("ElasticNet - MSE:", mse_elastic_net, "MAE:", mae_elastic_net, "R-squared:", r2_elastic_net, "Adj R-squared:", adj_r2_elastic_net)
print("Random Forest - MSE:", mse_rf, "MAE:", mae_rf, "R-squared:", r2_rf, "Adj R-squared:", adj_r2_rf)
print("SVR - MSE:", mse_svr, "MAE:", mae_svr, "R-squared:", r2_svr, "Adj R-squared:", adj_r2_svr)

